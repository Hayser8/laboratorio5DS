{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d58f9bfe",
   "metadata": {},
   "source": [
    "## **Laboratorio 5**\n",
    "- Sofía García - 22210\n",
    "- Joaquín Campos - 22155\n",
    "- Julio García Salas - 22076"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63833b38",
   "metadata": {},
   "source": [
    "## **Inciso 1 y 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b64c8cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versions -> pandas: 2.2.3\n",
      "✔️  Cargado: train.csv  -> shape=(7613, 5)\n",
      "✔️  Cargado: test.csv  -> shape=(3263, 4)\n",
      "🧭 Columna de texto detectada: 'text'\n",
      "🧭 Columna de etiqueta detectada: 'target'\n",
      "\n",
      "=== DESCRIPCIÓN GENERAL (train) ===\n",
      "Shape: (7613, 5)\n",
      "\n",
      "Columnas y tipos:\n",
      " id           int64\n",
      "keyword     object\n",
      "location    object\n",
      "text        object\n",
      "target       int64\n",
      "dtype: object\n",
      "\n",
      "Valores nulos por columna:\n",
      "id             0\n",
      "keyword       61\n",
      "location    2533\n",
      "text           0\n",
      "target         0\n",
      "dtype: int64\n",
      "\n",
      "Filas duplicadas (todas las columnas): 0\n",
      "Filas con 'text' duplicado: 110\n",
      "\n",
      "Distribución de clases:\n",
      "target\n",
      "0    4342\n",
      "1    3271\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Proporción de clases:\n",
      "target\n",
      "0    0.57\n",
      "1    0.43\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "=== Estadísticas de longitud de texto ===\n",
      "Caracteres -> mean: 101.0 | std: 33.8 | min: 7 | p50: 107 | p95: 140 | max: 157\n",
      "Tokens     -> mean: 14.9 | std: 5.7 | min: 1 | p50: 15 | p95: 24 | max: 31\n",
      "\n",
      "=== MUESTRAS DE TEXTO (por clase si aplica) ===\n",
      "\n",
      ">> Clase = 0 (muestras):\n",
      "- Everyday is a near death fatality for me on the road. Thank god is on my side.??\n",
      "- #Lifestyle Û÷It makes me sickÛª: Baby clothes deemed a Û÷hazardÛª http://t.co/0XrfVidxA2 http://t.co/oIHwgEZDCk\n",
      "- @Lenn_Len Probably. We are inundated with them most years!\n",
      "\n",
      ">> Clase = 1 (muestras):\n",
      "- Nearly 50 thousand people affected by floods in #Paraguay ? http://t.co/aw23wXtyjB http://t.co/ABgct9VFUa\n",
      "- Vladimir Putin Issues Major Warning But Is It Too Late To Escape Armageddon? http://t.co/gBxafy1m1C\n",
      "- @DoctorFluxx @StefanEJones @spinnellii @themermacorn No burning buildings and rob during a riot. That's embarrassing &amp; ruining this nation.\n",
      "\n",
      "=== PREVIEW test.csv ===\n",
      "Shape: (3263, 4)\n",
      "Columnas: ['id', 'keyword', 'location', 'text']\n",
      "\n",
      "Ejemplos test:\n",
      "- Just happened a terrible car crash\n",
      "- Heard about #earthquake is different cities, stay safe everyone.\n",
      "- there is a forest fire at spot pond, geese are fleeing across the street, I cannot save them all\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "\n",
    "print(\"Versions -> pandas:\", pd.__version__)\n",
    "\n",
    "# ---- 1) Rutas candidatas (usa la que tengas local) ----\n",
    "TRAIN_CANDIDATES = [\"train.csv\", \"./train.csv\", \"/mnt/data/train.csv\"]\n",
    "TEST_CANDIDATES  = [\"test.csv\", \"./test.csv\", \"/mnt/data/test.csv\"]\n",
    "\n",
    "def load_first_available(paths):\n",
    "    last_err = None\n",
    "    for p in paths:\n",
    "        try:\n",
    "            df = pd.read_csv(p)\n",
    "            print(f\"✔️  Cargado: {p}  -> shape={df.shape}\")\n",
    "            return df, p\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "    raise FileNotFoundError(f\"No se pudo cargar desde {paths}. Último error: {last_err}\")\n",
    "\n",
    "train_df, train_path = load_first_available(TRAIN_CANDIDATES)\n",
    "\n",
    "# test.csv es opcional para este paso\n",
    "try:\n",
    "    test_df, test_path = load_first_available(TEST_CANDIDATES)\n",
    "except Exception:\n",
    "    test_df, test_path = None, None\n",
    "    print(\"ℹ️  test.csv no encontrado (no es obligatorio para este inciso).\")\n",
    "\n",
    "# ---- 2) Intento de detección de columnas clave ----\n",
    "def guess_text_col(df):\n",
    "    candidates = [\"text\", \"tweet\", \"content\", \"message\", \"Text\", \"Tweet\"]\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    # fallback: columna object con mayor longitud media\n",
    "    obj_cols = [c for c in df.columns if df[c].dtype == \"object\"]\n",
    "    if not obj_cols:\n",
    "        return None\n",
    "    def avg_len(s):\n",
    "        try:\n",
    "            return s.dropna().astype(str).str.len().mean()\n",
    "        except Exception:\n",
    "            return -1\n",
    "    best = max(obj_cols, key=lambda c: avg_len(df[c]))\n",
    "    return best\n",
    "\n",
    "def guess_target_col(df):\n",
    "    candidates = [\"target\", \"label\", \"is_disaster\", \"Target\", \"Label\"]\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "text_col   = guess_text_col(train_df)\n",
    "target_col = guess_target_col(train_df)\n",
    "\n",
    "print(f\"🧭 Columna de texto detectada: {text_col!r}\")\n",
    "print(f\"🧭 Columna de etiqueta detectada: {target_col!r}\")\n",
    "\n",
    "# ---- 3) Descripción general del train ----\n",
    "print(\"\\n=== DESCRIPCIÓN GENERAL (train) ===\")\n",
    "print(\"Shape:\", train_df.shape)\n",
    "print(\"\\nColumnas y tipos:\\n\", train_df.dtypes)\n",
    "\n",
    "print(\"\\nValores nulos por columna:\")\n",
    "print(train_df.isna().sum())\n",
    "\n",
    "# Duplicados (por fila completa y por texto)\n",
    "dup_rows = train_df.duplicated().sum()\n",
    "print(f\"\\nFilas duplicadas (todas las columnas): {dup_rows}\")\n",
    "\n",
    "if text_col is not None:\n",
    "    dup_texts = train_df.duplicated(subset=[text_col]).sum()\n",
    "    print(f\"Filas con {text_col!r} duplicado: {dup_texts}\")\n",
    "\n",
    "# Distribución de clases (si hay etiqueta)\n",
    "if target_col is not None:\n",
    "    print(\"\\nDistribución de clases:\")\n",
    "    print(train_df[target_col].value_counts(dropna=False))\n",
    "    print(\"\\nProporción de clases:\")\n",
    "    print(train_df[target_col].value_counts(normalize=True, dropna=False).round(3))\n",
    "\n",
    "# Longitud de textos\n",
    "if text_col is not None:\n",
    "    tmp = train_df[text_col].astype(str)\n",
    "    char_len = tmp.str.len()\n",
    "    token_len = tmp.str.split().apply(len)\n",
    "\n",
    "    print(\"\\n=== Estadísticas de longitud de texto ===\")\n",
    "    print(\"Caracteres -> mean:\", round(char_len.mean(), 1),\n",
    "          \"| std:\", round(char_len.std(), 1),\n",
    "          \"| min:\", int(char_len.min()),\n",
    "          \"| p50:\", int(char_len.median()),\n",
    "          \"| p95:\", int(char_len.quantile(0.95)),\n",
    "          \"| max:\", int(char_len.max()))\n",
    "    print(\"Tokens     -> mean:\", round(token_len.mean(), 1),\n",
    "          \"| std:\", round(token_len.std(), 1),\n",
    "          \"| min:\", int(token_len.min()),\n",
    "          \"| p50:\", int(token_len.median()),\n",
    "          \"| p95:\", int(token_len.quantile(0.95)),\n",
    "          \"| max:\", int(token_len.max()))\n",
    "\n",
    "# Muestra rápida de ejemplos por clase (si hay etiqueta)\n",
    "def sample_by_class(df, label_col, k=3):\n",
    "    out = {}\n",
    "    if label_col is None:\n",
    "        return out\n",
    "    for cls, group in df.groupby(label_col):\n",
    "        out[cls] = group.sample(n=min(k, len(group)), random_state=42)\n",
    "    return out\n",
    "\n",
    "print(\"\\n=== MUESTRAS DE TEXTO (por clase si aplica) ===\")\n",
    "if target_col is not None and text_col is not None:\n",
    "    samples = sample_by_class(train_df, target_col, k=3)\n",
    "    for cls, df_s in samples.items():\n",
    "        print(f\"\\n>> Clase = {cls} (muestras):\")\n",
    "        for i, row in df_s.iterrows():\n",
    "            txt = str(row[text_col])\n",
    "            txt = re.sub(r\"\\s+\", \" \", txt).strip()\n",
    "            print(\"-\", txt[:200] + (\"...\" if len(txt) > 200 else \"\"))\n",
    "else:\n",
    "    # si no hay target, solo mostramos algunas filas\n",
    "    print(train_df.head(5))\n",
    "\n",
    "# ---- 4) (Opcional) vista previa del test ----\n",
    "if test_df is not None:\n",
    "    print(\"\\n=== PREVIEW test.csv ===\")\n",
    "    print(\"Shape:\", test_df.shape)\n",
    "    print(\"Columnas:\", list(test_df.columns)[:10])\n",
    "    if text_col and text_col in test_df.columns:\n",
    "        print(\"\\nEjemplos test:\")\n",
    "        for t in test_df[text_col].astype(str).head(3):\n",
    "            t = re.sub(r\"\\s+\", \" \", t).strip()\n",
    "            print(\"-\", t[:200] + (\"...\" if len(t) > 200 else \"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0447387d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando columnas -> texto: 'text' | etiqueta: 'target'\n",
      "\n",
      "=== REPORTE DE PREPROCESAMIENTO (train) ===\n",
      "Filas procesadas: 7613\n",
      "Totales removidos/detectados:\n",
      " • URLs:      4723\n",
      " • Menciones: 2715\n",
      " • Hashtags:  3330 (se conservó la palabra, sin '#')\n",
      " • Emojis:    4888\n",
      "\n",
      "Longitud de tokens (antes vs después):\n",
      " • Tokens (orig)  -> mean: 14.9 | p50: 15 | p95: 24\n",
      " • Tokens (clean) -> mean: 8.2 | p50: 8 | p95: 14\n",
      "\n",
      "Tamaño de texto limpio por clase (número de tokens):\n",
      "        mean  median  max  min\n",
      "target                        \n",
      "0       7.77     8.0   20    0\n",
      "1       8.77     9.0   21    0\n",
      "\n",
      ">> Ejemplos clase=0:\n",
      "- RAW  : Everyday is a near death fatality for me on the road. Thank god is on my side.??\n",
      "  CLEAN: everyday near death fatality road thank god\n",
      "- RAW  : #Lifestyle Û÷It makes me sickÛª: Baby clothes deemed a Û÷hazardÛª http://t.co/0XrfVidxA2 http://t.co/oIHwgEZDCk\n",
      "  CLEAN: lifestyle û÷it makes sickûa baby clothes deemed û÷hazardûa\n",
      "- RAW  : @Lenn_Len Probably. We are inundated with them most years!\n",
      "  CLEAN: probably inundated years\n",
      "\n",
      ">> Ejemplos clase=1:\n",
      "- RAW  : Nearly 50 thousand people affected by floods in #Paraguay ? http://t.co/aw23wXtyjB http://t.co/ABgct9VFUa\n",
      "  CLEAN: nearly thousand people affected floods paraguay\n",
      "- RAW  : Vladimir Putin Issues Major Warning But Is It Too Late To Escape Armageddon? http://t.co/gBxafy1m1C\n",
      "  CLEAN: vladimir putin issues major warning late escape armageddon\n",
      "- RAW  : @DoctorFluxx @StefanEJones @spinnellii @themermacorn  No burning buildings and rob during a riot. That's embarrassing &amp; ruining this nation.\n",
      "  CLEAN: burning buildings rob riot thats embarrassing ruining nation\n",
      "\n",
      "Publicaciones que quedaron vacías tras limpieza: 4\n",
      "\n",
      "=== Vista previa columnas limpias ===\n",
      "                                                text  \\\n",
      "0  Our Deeds are the Reason of this #earthquake M...   \n",
      "1             Forest fire near La Ronge Sask. Canada   \n",
      "2  All residents asked to 'shelter in place' are ...   \n",
      "3  13,000 people receive #wildfires evacuation or...   \n",
      "4  Just got sent this photo from Ruby #Alaska as ...   \n",
      "\n",
      "                                          text_clean  \n",
      "0              deeds reason earthquake allah forgive  \n",
      "1                   forest near la ronge sask canada  \n",
      "2  residents asked shelter place notified officer...  \n",
      "3  people receive wildfires evacuation orders cal...  \n",
      "4  just got sent photo ruby alaska smoke wildfire...  \n"
     ]
    }
   ],
   "source": [
    "# Cell 2 — Preprocesamiento detallado (inciso 3) — versión corregida\n",
    "\n",
    "import re, html, unicodedata, string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Asegurarnos de tener train_df, text_col y target_col (del Cell 1) ---\n",
    "if \"train_df\" not in globals():\n",
    "    train_df = pd.read_csv(\"train.csv\")\n",
    "if \"text_col\" not in globals():\n",
    "    text_col = \"text\" if \"text\" in train_df.columns else train_df.select_dtypes(\"object\").columns[0]\n",
    "if \"target_col\" not in globals():\n",
    "    target_col = \"target\" if \"target\" in train_df.columns else None\n",
    "\n",
    "print(f\"Usando columnas -> texto: {text_col!r} | etiqueta: {target_col!r}\")\n",
    "\n",
    "# --- Stopwords en inglés (evitamos dependencias de descarga) ---\n",
    "try:\n",
    "    from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "    STOPWORDS = set(ENGLISH_STOP_WORDS)\n",
    "except Exception:\n",
    "    # Fallback mínimo si sklearn no está disponible\n",
    "    STOPWORDS = set(\"\"\"\n",
    "a about above after again against all am an and any are as at be because been before being below between both\n",
    "but by can did do does doing down during each few for from further had has have having he she'd he'll she's her\n",
    "here hers herself him himself his how i i'd i'll i'm i've if in into is it it's its itself let me more most my\n",
    "myself no nor not of off on once only or other our ours ourselves out over own same she should so some such\n",
    "than that that's the their theirs them themselves then there there's these they they'd they'll they're they've\n",
    "this those through to too under until up very was we we'd we'll we're we've were what what's when when's where\n",
    "where's which while who who's whom why why's with you you'd you'll you're you've your yours yourself yourselves\n",
    "\"\"\".split())\n",
    "\n",
    "# Ajustes de stopwords específicos de tweets\n",
    "STOPWORDS |= {\"rt\", \"amp\", \"https\", \"http\", \"co\", \"t\"}  # &amp; y tokens comunes de URLs acortadas\n",
    "\n",
    "# --- Compilar patrones regex reutilizables ---\n",
    "URL_RE        = re.compile(r\"(https?://\\S+|www\\.\\S+)\", flags=re.IGNORECASE)\n",
    "MENTION_RE    = re.compile(r\"@\\w+\")\n",
    "HASHTAG_RE    = re.compile(r\"#(\\w+)\")\n",
    "HTML_ENT_RE   = re.compile(r\"&[a-z]+;\")\n",
    "EMOTICON_RE   = re.compile(r\"(?::|;|=|8)(?:-|'|o)?(?:\\)|\\(|D|P|/|\\\\|\\||\\]|\\[)\", flags=re.IGNORECASE)\n",
    "\n",
    "# Rango amplio de emojis (unicode)\n",
    "EMOJI_RE = re.compile(\n",
    "    \"[\" +\n",
    "    \"\\U0001F300-\\U0001F5FF\" +  # pictos y símbolos\n",
    "    \"\\U0001F600-\\U0001F64F\" +  # emoticonos\n",
    "    \"\\U0001F680-\\U0001F6FF\" +  # transporte\n",
    "    \"\\U0001F700-\\U0001F77F\" +\n",
    "    \"\\U0001F780-\\U0001F7FF\" +\n",
    "    \"\\U0001F800-\\U0001F8FF\" +\n",
    "    \"\\U0001F900-\\U0001F9FF\" +\n",
    "    \"\\U0001FA00-\\U0001FA6F\" +\n",
    "    \"\\U0001FA70-\\U0001FAFF\" +\n",
    "    \"\\U00002700-\\U000027BF\" +  # dingbats\n",
    "    \"\\U00002600-\\U000026FF\" +  # misceláneo\n",
    "    \"]+\",\n",
    "    flags=re.UNICODE\n",
    ")\n",
    "\n",
    "PUNCT_TABLE = str.maketrans(\"\", \"\", string.punctuation)  # elimina puntuación ASCII\n",
    "\n",
    "def normalize_unicode(text: str) -> str:\n",
    "    # Normaliza a NFKC para unificar formas y símbolos \"raros\"\n",
    "    return unicodedata.normalize(\"NFKC\", text)\n",
    "\n",
    "def preprocess_text(s: str, keep_number_token=\"911\"):\n",
    "    orig = str(s)\n",
    "\n",
    "    # Métricas de sucio\n",
    "    urls_found     = len(URL_RE.findall(orig))\n",
    "    mentions_found = len(MENTION_RE.findall(orig))\n",
    "    hashtags_found = len(HASHTAG_RE.findall(orig))\n",
    "    emojis_found   = len(EMOJI_RE.findall(orig)) + len(EMOTICON_RE.findall(orig))\n",
    "\n",
    "    # 1) Minúsculas + desenmascarar HTML (&amp; -> &)\n",
    "    t = html.unescape(orig).lower()\n",
    "\n",
    "    # 2) Normalización unicode\n",
    "    t = normalize_unicode(t)\n",
    "\n",
    "    # 3) Eliminar URLs completas\n",
    "    t = URL_RE.sub(\" \", t)\n",
    "\n",
    "    # 4) Eliminar @menciones\n",
    "    t = MENTION_RE.sub(\" \", t)\n",
    "\n",
    "    # 5) Mantener el texto del hashtag, quitando el '#'\n",
    "    #    \"#wildfire\" -> \"wildfire\"\n",
    "    t = HASHTAG_RE.sub(r\"\\1\", t)\n",
    "\n",
    "    # 6) Eliminar emojis y emoticones\n",
    "    t = EMOJI_RE.sub(\" \", t)\n",
    "    t = EMOTICON_RE.sub(\" \", t)\n",
    "\n",
    "    # 7) Eliminar puntuación ASCII (quita apóstrofes, comas, etc.)\n",
    "    t = t.translate(PUNCT_TABLE)\n",
    "\n",
    "    # 8) Quitar residuos de entidades HTML restantes\n",
    "    t = HTML_ENT_RE.sub(\" \", t)\n",
    "\n",
    "    # 9) Tokenizar por espacios\n",
    "    tokens = [tok for tok in t.split() if tok]\n",
    "\n",
    "    # 10) Quitar stopwords\n",
    "    tokens = [tok for tok in tokens if tok not in STOPWORDS]\n",
    "\n",
    "    # 11) Manejo de números:\n",
    "    #     - Eliminamos tokens NUMÉRICOS EXCEPTO \"911\" (mantener porque puede ser relevante en desastres)\n",
    "    #     - Para tokens alfanuméricos, conservamos tal cual (e.g., \"h2o\", \"m5.0\")\n",
    "    cleaned_tokens = []\n",
    "    for tok in tokens:\n",
    "        if tok.isdigit():\n",
    "            if keep_number_token and tok == keep_number_token:\n",
    "                cleaned_tokens.append(tok)\n",
    "            # si es otro número puro, lo omitimos\n",
    "        else:\n",
    "            cleaned_tokens.append(tok)\n",
    "\n",
    "    # 12) Reconstrucción\n",
    "    clean_text = \" \".join(cleaned_tokens)\n",
    "\n",
    "    # Métricas post\n",
    "    return {\n",
    "        \"clean_text\": clean_text,\n",
    "        \"tokens\": cleaned_tokens,\n",
    "        \"n_urls\": urls_found,\n",
    "        \"n_mentions\": mentions_found,\n",
    "        \"n_hashtags\": hashtags_found,\n",
    "        \"n_emojis\": emojis_found,\n",
    "        \"orig_len_chars\": len(orig),\n",
    "        \"orig_len_tokens\": len(orig.split()),\n",
    "        \"clean_len_tokens\": len(cleaned_tokens),\n",
    "    }\n",
    "\n",
    "# --- Aplicar preprocesamiento ---\n",
    "proc = train_df[text_col].astype(str).apply(preprocess_text)\n",
    "\n",
    "train_df[\"text_clean\"]   = proc.apply(lambda x: x[\"clean_text\"])\n",
    "train_df[\"tokens_clean\"] = proc.apply(lambda x: x[\"tokens\"])\n",
    "\n",
    "# Métricas agregadas del proceso\n",
    "report = pd.DataFrame({\n",
    "    \"urls\":         proc.apply(lambda x: x[\"n_urls\"]),\n",
    "    \"mentions\":     proc.apply(lambda x: x[\"n_mentions\"]),\n",
    "    \"hashtags\":     proc.apply(lambda x: x[\"n_hashtags\"]),\n",
    "    \"emojis\":       proc.apply(lambda x: x[\"n_emojis\"]),\n",
    "    \"orig_chars\":   proc.apply(lambda x: x[\"orig_len_chars\"]),\n",
    "    \"orig_tokens\":  proc.apply(lambda x: x[\"orig_len_tokens\"]),\n",
    "    \"clean_tokens\": proc.apply(lambda x: x[\"clean_len_tokens\"]),\n",
    "})\n",
    "\n",
    "# --- Reporte en consola ---\n",
    "print(\"\\n=== REPORTE DE PREPROCESAMIENTO (train) ===\")\n",
    "print(f\"Filas procesadas: {len(train_df)}\")\n",
    "print(\"Totales removidos/detectados:\")\n",
    "print(\" • URLs:     \", int(report[\"urls\"].sum()))\n",
    "print(\" • Menciones:\", int(report[\"mentions\"].sum()))\n",
    "print(\" • Hashtags: \", int(report[\"hashtags\"].sum()), \"(se conservó la palabra, sin '#')\")\n",
    "print(\" • Emojis:   \", int(report[\"emojis\"].sum()))\n",
    "\n",
    "print(\"\\nLongitud de tokens (antes vs después):\")\n",
    "print(\" • Tokens (orig)  -> mean:\", round(report[\"orig_tokens\"].mean(),1),\n",
    "      \"| p50:\", int(report[\"orig_tokens\"].median()),\n",
    "      \"| p95:\", int(report[\"orig_tokens\"].quantile(0.95)))\n",
    "print(\" • Tokens (clean) -> mean:\", round(report[\"clean_tokens\"].mean(),1),\n",
    "      \"| p50:\", int(report[\"clean_tokens\"].median()),\n",
    "      \"| p95:\", int(report[\"clean_tokens\"].quantile(0.95)))\n",
    "\n",
    "# Distribución por clase (si hay etiqueta) — usando longitud de tokens limpios\n",
    "if target_col is not None and target_col in train_df.columns:\n",
    "    train_df[\"tok_len\"] = train_df[\"tokens_clean\"].apply(len)\n",
    "    by_cls = train_df.groupby(target_col)[\"tok_len\"].agg([\"mean\",\"median\",\"max\",\"min\"])\n",
    "    print(\"\\nTamaño de texto limpio por clase (número de tokens):\")\n",
    "    print(by_cls.round(2))\n",
    "\n",
    "# --- Ejemplos antes/después por clase (para documentación del inciso 3) ---\n",
    "def show_examples(df, k=3):\n",
    "    groups = df.groupby(target_col) if (target_col is not None and target_col in df.columns) else [(None, df)]\n",
    "    for cls_val, g in groups:\n",
    "        print(f\"\\n>> Ejemplos clase={cls_val}:\")\n",
    "        samp = g.sample(n=min(k, len(g)), random_state=42)\n",
    "        for _, r in samp.iterrows():\n",
    "            raw = str(r[text_col]).strip().replace(\"\\n\", \" \")\n",
    "            clean = str(r[\"text_clean\"]).strip()\n",
    "            print(\"- RAW  :\", raw[:160] + (\"...\" if len(raw) > 160 else \"\"))\n",
    "            print(\"  CLEAN:\", clean[:160] + (\"...\" if len(clean) > 160 else \"\"))\n",
    "\n",
    "show_examples(train_df, k=3)\n",
    "\n",
    "# --- Chequeo rápido de vacíos tras limpieza (p.ej. posts que quedan sin tokens) ---\n",
    "empties = train_df[\"text_clean\"].fillna(\"\").str.strip().eq(\"\")\n",
    "n_empties = int(empties.sum())\n",
    "print(f\"\\nPublicaciones que quedaron vacías tras limpieza: {n_empties}\")\n",
    "\n",
    "# --- Vista previa final ---\n",
    "print(\"\\n=== Vista previa columnas limpias ===\")\n",
    "print(train_df[[text_col, \"text_clean\"]].head(5))\n",
    "\n",
    "# Nota: NO eliminamos duplicados aquí para no alterar el dataset base;\n",
    "# si lo necesitas para el modelo, podemos hacerlo en el inciso del modelado.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56dcc9f",
   "metadata": {},
   "source": [
    "\n",
    "# Inciso 3 — Análisis del preprocesamiento (train.csv)\n",
    "\n",
    "**Columnas usadas** → texto: `text` | etiqueta: `target`  \n",
    "**Filas procesadas**: 7,613\n",
    "\n",
    "---\n",
    "\n",
    "## 1) Qué se limpió/eliminó\n",
    "\n",
    "- **URLs** detectadas: **4,723**  \n",
    "- **@Menciones**: **2,715**  \n",
    "- **#Hashtags**: **3,330** *(se conserva la palabra, sin ‘#’)*  \n",
    "- **Emojis/Emoticones**: **4,888**\n",
    "\n",
    "> Comentario: El dataset tiene una **alta carga social** (muchas URLs, menciones y emojis). Limpiar estos elementos ayuda a reducir ruido y sesgos hacia la forma de escritura en Twitter sin eliminar señales semánticas clave (p. ej., *wildfire*, *evacuation*, *floods*).\n",
    "\n",
    "---\n",
    "\n",
    "## 2) Longitud de texto (antes vs después)\n",
    "\n",
    "- **Tokens (original)** → *mean*: **14.9**, *p50*: **15**, *p95*: **24**  \n",
    "- **Tokens (limpio)** → *mean*: **8.2**, *p50*: **8**, *p95*: **14**\n",
    "\n",
    "> Interpretación: tras la limpieza, el texto queda **~45% más corto** en promedio. Esto sugiere que gran parte del contenido eran conectores, signos, URLs y marcas sociales. La reducción es esperable y suele **mejorar la densidad semántica** por token.\n",
    "\n",
    "---\n",
    "\n",
    "## 3) Longitud por clase (tokens limpios)\n",
    "\n",
    "| target | mean | median | max | min |\n",
    "|:------:|-----:|-------:|----:|----:|\n",
    "|   0    | 7.77 |  8.00  | 20  |  0  |\n",
    "|   1    | 8.77 |  9.00  | 21  |  0  |\n",
    "\n",
    "> Observación: Los tweets **etiquetados como desastre (1)** son, en promedio, **~1 token más largos**. No es una diferencia enorme, pero puede indicar que los tweets de desastres incluyen **más contexto** (p. ej., lugar + evento + afectación).\n",
    "\n",
    "---\n",
    "\n",
    "## 4) Ejemplos ilustrativos (antes → después)\n",
    "\n",
    "**Clase 0 (no desastre):**\n",
    "- *everyday is a near death fatality…* → `everyday near death fatality road thank god`  \n",
    "  - Riesgo de **falsos positivos**: palabras de alto “drama” (*death*, *fatality*) en contextos figurados o hiperbolizados.\n",
    "- *#Lifestyle… hazard* → `lifestyle … makes sick … baby clothes deemed … hazard`  \n",
    "  - Muestra ruido de codificación (ver “mojibake” abajo).\n",
    "- *@Lenn_Len… inundated* → `probably inundated years`  \n",
    "  - *inundated* aquí no describe una **inundación real**, sino “abrumados” → **ambigüedad semántica**.\n",
    "\n",
    "**Clase 1 (desastre):**\n",
    "- *…floods in #Paraguay* → `nearly thousand people affected floods paraguay`  \n",
    "  - Léxico fuerte: *affected*, *floods*, **topónimo** (*paraguay*).\n",
    "- *…escape Armageddon* → `vladimir putin issues major warning late escape armageddon`  \n",
    "  - Palabras catastróficas (no siempre desastre natural, pero señalan severidad).\n",
    "- *@… burning buildings… riot* → `burning buildings rob riot thats embarrassing ruining nation`  \n",
    "  - Léxico de **eventos violentos/daño** (*burning*, *buildings*, *riot*).\n",
    "\n",
    "---\n",
    "\n",
    "## 5) Calidad de limpieza: hallazgos y mejoras sugeridas\n",
    "\n",
    "- **4 publicaciones vacías** tras limpieza → recomendación: *drop* o marcar para tratamiento especial (no aportan señal textual).  \n",
    "- **Mojibake / artefactos Unicode** (p. ej., `û÷`) persisten en algunos textos.\n",
    "  - *Sugerencia*: añadir una etapa opcional para **filtrar caracteres no alfabéticos** fuera de rangos comunes o normalizar a ASCII cuando el idioma sea inglés:\n",
    "    - Ej.: `re.sub(r\"[^a-z0-9\\s\\-]\", \" \", text)` tras NFKC (cuidar no perder topónimos con tildes si hubiera idioma mixto).\n",
    "- **Números**: se conservó **“911”** y se eliminaron otros números puros.\n",
    "  - *Sugerencia*: además de “911”, puede valer la pena **conservar números decimales** y patrones de magnitud (*“5.8”*, *“m5.0”*) por su relación con **terremotos**.\n",
    "  - Regla posible: mantener `r\"\\d+\\.\\d+\"` (decimales) y tokens con letras+digits (*m5.0, h2o* ya se conservan).\n",
    "\n",
    "---\n",
    "\n",
    "## 6) Implicaciones para unigramas/bigramas (inciso 4)\n",
    "\n",
    "- **Unigramas** útiles (intuición por ejemplos): `wildfire`, `evacuation`, `shelter`, `floods`, `smoke`, `burning`, `evacuate`, `earthquake`, `aftershock`, `landslide`, `tornado`, `explosion`, `derailment`, `casualties`, `emergency`, `fatalities`, `rescue`.  \n",
    "- **Bigrams** muy recomendables para **contexto**:\n",
    "  - `forest fire`, `car crash`, `shelter in`, `in place`, `evacuation orders`, `burning buildings`, `state emergency`, `death toll`, `flash flood`, `wildfire smoke`.  \n",
    "  - También **topónimo + evento**: `paraguay floods`, `alaska wildfire`, etc.\n",
    "- **Trigrams** puntuales: `shelter in place`, `state of emergency`.  \n",
    "  - Útiles pero más escasos; probar **bigrams primero** y evaluar mejora.\n",
    "\n",
    "> Conclusión: sí **vale la pena explorar bigramas** (y unos pocos trigrams clave) para capturar **frases indicadoras** que desambigüen palabras sueltas.\n",
    "\n",
    "---\n",
    "\n",
    "## 7) Palabras con riesgo de confusión (discutir en el reporte)\n",
    "\n",
    "- **Hiperboles cotidianas** (*“near death”*, *“bombed this exam”*, *“my phone exploded”*): pueden **sobrerreaccionar** en unigramas.  \n",
    "  - Bigrams/trigrams y señales **de contexto real** (lugares, cifras, verbos de reporte: *evacuate, rescue, declare*) ayudan a reducir falsos positivos.\n",
    "- **Polisemia** (*inundated* = abrumado vs inundado real).  \n",
    "  - **Topónimos**, **fechas/horas**, **números de víctimas**, y palabras de **autoridad** (*officials, declared, warning, alert*) tienden a correlacionar mejor con eventos reales.\n",
    "\n",
    "---\n",
    "\n",
    "## 8) Recomendaciones prácticas para el modelo preliminar\n",
    "\n",
    "1. **Features**: TF–IDF **unigramas+bigramas** con min_df bajo (p. ej., 2–5) y `max_features` razonable (20k–50k), n-gram range `(1,2)`.  \n",
    "2. **Clase desbalanceada moderada** (57/43): probar **calibración de clase** (`class_weight='balanced'` en modelos lineales) y **AUC/F1** como métricas.  \n",
    "3. **Deduplicación opcional**: hay **110 textos duplicados**; considerar *drop* para evitar sesgos en validación.  \n",
    "4. **Validación**: *stratified split* y baseline con **Logistic Regression** / **Linear SVM**; comparar con Naive Bayes.  \n",
    "5. **Top tokens por clase**: antes del modelo, obtener **frecuencias por clase** y **nubes de palabras** para documentar (inciso 4 y 5).\n",
    "\n",
    "---\n",
    "\n",
    "## 9) Vista previa (sanity check)\n",
    "\n",
    "Se observa que el **contenido semántico central** se conserva, p. ej.:  \n",
    "- `deeds reason earthquake allah forgive`  \n",
    "- `residents asked shelter place …`  \n",
    "- `people receive wildfires evacuation orders california …`  \n",
    "- `smoke wildfire … alaska …`  \n",
    "\n",
    "> Esto confirma que la limpieza **respeta los términos clave** asociados a desastres y reduce ruido social.\n",
    "\n",
    "---\n",
    "\n",
    "### Próximo paso\n",
    "Proceder con **inciso 4**: conteos de frecuencia **por clase** (unigramas y bigramas), discusión de términos útiles y visualizaciones (nube de palabras + histogramas).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
