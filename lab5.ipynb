{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d58f9bfe",
   "metadata": {},
   "source": [
    "## **Laboratorio 5**\n",
    "- Sof√≠a Garc√≠a - 22210\n",
    "- Joaqu√≠n Campos - 22155\n",
    "- Julio Garc√≠a Salas - 22076"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63833b38",
   "metadata": {},
   "source": [
    "## **Inciso 1 y 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b64c8cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versions -> pandas: 2.2.3\n",
      "‚úîÔ∏è  Cargado: train.csv  -> shape=(7613, 5)\n",
      "‚úîÔ∏è  Cargado: test.csv  -> shape=(3263, 4)\n",
      "üß≠ Columna de texto detectada: 'text'\n",
      "üß≠ Columna de etiqueta detectada: 'target'\n",
      "\n",
      "=== DESCRIPCI√ìN GENERAL (train) ===\n",
      "Shape: (7613, 5)\n",
      "\n",
      "Columnas y tipos:\n",
      " id           int64\n",
      "keyword     object\n",
      "location    object\n",
      "text        object\n",
      "target       int64\n",
      "dtype: object\n",
      "\n",
      "Valores nulos por columna:\n",
      "id             0\n",
      "keyword       61\n",
      "location    2533\n",
      "text           0\n",
      "target         0\n",
      "dtype: int64\n",
      "\n",
      "Filas duplicadas (todas las columnas): 0\n",
      "Filas con 'text' duplicado: 110\n",
      "\n",
      "Distribuci√≥n de clases:\n",
      "target\n",
      "0    4342\n",
      "1    3271\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Proporci√≥n de clases:\n",
      "target\n",
      "0    0.57\n",
      "1    0.43\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "=== Estad√≠sticas de longitud de texto ===\n",
      "Caracteres -> mean: 101.0 | std: 33.8 | min: 7 | p50: 107 | p95: 140 | max: 157\n",
      "Tokens     -> mean: 14.9 | std: 5.7 | min: 1 | p50: 15 | p95: 24 | max: 31\n",
      "\n",
      "=== MUESTRAS DE TEXTO (por clase si aplica) ===\n",
      "\n",
      ">> Clase = 0 (muestras):\n",
      "- Everyday is a near death fatality for me on the road. Thank god is on my side.??\n",
      "- #Lifestyle ¬â√õ√∑It makes me sick¬â√õ¬™: Baby clothes deemed a ¬â√õ√∑hazard¬â√õ¬™ http://t.co/0XrfVidxA2 http://t.co/oIHwgEZDCk\n",
      "- @Lenn_Len Probably. We are inundated with them most years!\n",
      "\n",
      ">> Clase = 1 (muestras):\n",
      "- Nearly 50 thousand people affected by floods in #Paraguay ? http://t.co/aw23wXtyjB http://t.co/ABgct9VFUa\n",
      "- Vladimir Putin Issues Major Warning But Is It Too Late To Escape Armageddon? http://t.co/gBxafy1m1C\n",
      "- @DoctorFluxx @StefanEJones @spinnellii @themermacorn No burning buildings and rob during a riot. That's embarrassing &amp; ruining this nation.\n",
      "\n",
      "=== PREVIEW test.csv ===\n",
      "Shape: (3263, 4)\n",
      "Columnas: ['id', 'keyword', 'location', 'text']\n",
      "\n",
      "Ejemplos test:\n",
      "- Just happened a terrible car crash\n",
      "- Heard about #earthquake is different cities, stay safe everyone.\n",
      "- there is a forest fire at spot pond, geese are fleeing across the street, I cannot save them all\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "\n",
    "print(\"Versions -> pandas:\", pd.__version__)\n",
    "\n",
    "# ---- 1) Rutas candidatas (usa la que tengas local) ----\n",
    "TRAIN_CANDIDATES = [\"train.csv\", \"./train.csv\", \"/mnt/data/train.csv\"]\n",
    "TEST_CANDIDATES  = [\"test.csv\", \"./test.csv\", \"/mnt/data/test.csv\"]\n",
    "\n",
    "def load_first_available(paths):\n",
    "    last_err = None\n",
    "    for p in paths:\n",
    "        try:\n",
    "            df = pd.read_csv(p)\n",
    "            print(f\"‚úîÔ∏è  Cargado: {p}  -> shape={df.shape}\")\n",
    "            return df, p\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "    raise FileNotFoundError(f\"No se pudo cargar desde {paths}. √öltimo error: {last_err}\")\n",
    "\n",
    "train_df, train_path = load_first_available(TRAIN_CANDIDATES)\n",
    "\n",
    "# test.csv es opcional para este paso\n",
    "try:\n",
    "    test_df, test_path = load_first_available(TEST_CANDIDATES)\n",
    "except Exception:\n",
    "    test_df, test_path = None, None\n",
    "    print(\"‚ÑπÔ∏è  test.csv no encontrado (no es obligatorio para este inciso).\")\n",
    "\n",
    "# ---- 2) Intento de detecci√≥n de columnas clave ----\n",
    "def guess_text_col(df):\n",
    "    candidates = [\"text\", \"tweet\", \"content\", \"message\", \"Text\", \"Tweet\"]\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    # fallback: columna object con mayor longitud media\n",
    "    obj_cols = [c for c in df.columns if df[c].dtype == \"object\"]\n",
    "    if not obj_cols:\n",
    "        return None\n",
    "    def avg_len(s):\n",
    "        try:\n",
    "            return s.dropna().astype(str).str.len().mean()\n",
    "        except Exception:\n",
    "            return -1\n",
    "    best = max(obj_cols, key=lambda c: avg_len(df[c]))\n",
    "    return best\n",
    "\n",
    "def guess_target_col(df):\n",
    "    candidates = [\"target\", \"label\", \"is_disaster\", \"Target\", \"Label\"]\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "text_col   = guess_text_col(train_df)\n",
    "target_col = guess_target_col(train_df)\n",
    "\n",
    "print(f\"üß≠ Columna de texto detectada: {text_col!r}\")\n",
    "print(f\"üß≠ Columna de etiqueta detectada: {target_col!r}\")\n",
    "\n",
    "# ---- 3) Descripci√≥n general del train ----\n",
    "print(\"\\n=== DESCRIPCI√ìN GENERAL (train) ===\")\n",
    "print(\"Shape:\", train_df.shape)\n",
    "print(\"\\nColumnas y tipos:\\n\", train_df.dtypes)\n",
    "\n",
    "print(\"\\nValores nulos por columna:\")\n",
    "print(train_df.isna().sum())\n",
    "\n",
    "# Duplicados (por fila completa y por texto)\n",
    "dup_rows = train_df.duplicated().sum()\n",
    "print(f\"\\nFilas duplicadas (todas las columnas): {dup_rows}\")\n",
    "\n",
    "if text_col is not None:\n",
    "    dup_texts = train_df.duplicated(subset=[text_col]).sum()\n",
    "    print(f\"Filas con {text_col!r} duplicado: {dup_texts}\")\n",
    "\n",
    "# Distribuci√≥n de clases (si hay etiqueta)\n",
    "if target_col is not None:\n",
    "    print(\"\\nDistribuci√≥n de clases:\")\n",
    "    print(train_df[target_col].value_counts(dropna=False))\n",
    "    print(\"\\nProporci√≥n de clases:\")\n",
    "    print(train_df[target_col].value_counts(normalize=True, dropna=False).round(3))\n",
    "\n",
    "# Longitud de textos\n",
    "if text_col is not None:\n",
    "    tmp = train_df[text_col].astype(str)\n",
    "    char_len = tmp.str.len()\n",
    "    token_len = tmp.str.split().apply(len)\n",
    "\n",
    "    print(\"\\n=== Estad√≠sticas de longitud de texto ===\")\n",
    "    print(\"Caracteres -> mean:\", round(char_len.mean(), 1),\n",
    "          \"| std:\", round(char_len.std(), 1),\n",
    "          \"| min:\", int(char_len.min()),\n",
    "          \"| p50:\", int(char_len.median()),\n",
    "          \"| p95:\", int(char_len.quantile(0.95)),\n",
    "          \"| max:\", int(char_len.max()))\n",
    "    print(\"Tokens     -> mean:\", round(token_len.mean(), 1),\n",
    "          \"| std:\", round(token_len.std(), 1),\n",
    "          \"| min:\", int(token_len.min()),\n",
    "          \"| p50:\", int(token_len.median()),\n",
    "          \"| p95:\", int(token_len.quantile(0.95)),\n",
    "          \"| max:\", int(token_len.max()))\n",
    "\n",
    "# Muestra r√°pida de ejemplos por clase (si hay etiqueta)\n",
    "def sample_by_class(df, label_col, k=3):\n",
    "    out = {}\n",
    "    if label_col is None:\n",
    "        return out\n",
    "    for cls, group in df.groupby(label_col):\n",
    "        out[cls] = group.sample(n=min(k, len(group)), random_state=42)\n",
    "    return out\n",
    "\n",
    "print(\"\\n=== MUESTRAS DE TEXTO (por clase si aplica) ===\")\n",
    "if target_col is not None and text_col is not None:\n",
    "    samples = sample_by_class(train_df, target_col, k=3)\n",
    "    for cls, df_s in samples.items():\n",
    "        print(f\"\\n>> Clase = {cls} (muestras):\")\n",
    "        for i, row in df_s.iterrows():\n",
    "            txt = str(row[text_col])\n",
    "            txt = re.sub(r\"\\s+\", \" \", txt).strip()\n",
    "            print(\"-\", txt[:200] + (\"...\" if len(txt) > 200 else \"\"))\n",
    "else:\n",
    "    # si no hay target, solo mostramos algunas filas\n",
    "    print(train_df.head(5))\n",
    "\n",
    "# ---- 4) (Opcional) vista previa del test ----\n",
    "if test_df is not None:\n",
    "    print(\"\\n=== PREVIEW test.csv ===\")\n",
    "    print(\"Shape:\", test_df.shape)\n",
    "    print(\"Columnas:\", list(test_df.columns)[:10])\n",
    "    if text_col and text_col in test_df.columns:\n",
    "        print(\"\\nEjemplos test:\")\n",
    "        for t in test_df[text_col].astype(str).head(3):\n",
    "            t = re.sub(r\"\\s+\", \" \", t).strip()\n",
    "            print(\"-\", t[:200] + (\"...\" if len(t) > 200 else \"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0447387d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando columnas -> texto: 'text' | etiqueta: 'target'\n",
      "\n",
      "=== REPORTE DE PREPROCESAMIENTO (train) ===\n",
      "Filas procesadas: 7613\n",
      "Totales removidos/detectados:\n",
      " ‚Ä¢ URLs:      4723\n",
      " ‚Ä¢ Menciones: 2715\n",
      " ‚Ä¢ Hashtags:  3330 (se conserv√≥ la palabra, sin '#')\n",
      " ‚Ä¢ Emojis:    4888\n",
      "\n",
      "Longitud de tokens (antes vs despu√©s):\n",
      " ‚Ä¢ Tokens (orig)  -> mean: 14.9 | p50: 15 | p95: 24\n",
      " ‚Ä¢ Tokens (clean) -> mean: 8.2 | p50: 8 | p95: 14\n",
      "\n",
      "Tama√±o de texto limpio por clase (n√∫mero de tokens):\n",
      "        mean  median  max  min\n",
      "target                        \n",
      "0       7.77     8.0   20    0\n",
      "1       8.77     9.0   21    0\n",
      "\n",
      ">> Ejemplos clase=0:\n",
      "- RAW  : Everyday is a near death fatality for me on the road. Thank god is on my side.??\n",
      "  CLEAN: everyday near death fatality road thank god\n",
      "- RAW  : #Lifestyle ¬â√õ√∑It makes me sick¬â√õ¬™: Baby clothes deemed a ¬â√õ√∑hazard¬â√õ¬™ http://t.co/0XrfVidxA2 http://t.co/oIHwgEZDCk\n",
      "  CLEAN: lifestyle ¬â√ª√∑it makes sick¬â√ªa baby clothes deemed ¬â√ª√∑hazard¬â√ªa\n",
      "- RAW  : @Lenn_Len Probably. We are inundated with them most years!\n",
      "  CLEAN: probably inundated years\n",
      "\n",
      ">> Ejemplos clase=1:\n",
      "- RAW  : Nearly 50 thousand people affected by floods in #Paraguay ? http://t.co/aw23wXtyjB http://t.co/ABgct9VFUa\n",
      "  CLEAN: nearly thousand people affected floods paraguay\n",
      "- RAW  : Vladimir Putin Issues Major Warning But Is It Too Late To Escape Armageddon? http://t.co/gBxafy1m1C\n",
      "  CLEAN: vladimir putin issues major warning late escape armageddon\n",
      "- RAW  : @DoctorFluxx @StefanEJones @spinnellii @themermacorn  No burning buildings and rob during a riot. That's embarrassing &amp; ruining this nation.\n",
      "  CLEAN: burning buildings rob riot thats embarrassing ruining nation\n",
      "\n",
      "Publicaciones que quedaron vac√≠as tras limpieza: 4\n",
      "\n",
      "=== Vista previa columnas limpias ===\n",
      "                                                text  \\\n",
      "0  Our Deeds are the Reason of this #earthquake M...   \n",
      "1             Forest fire near La Ronge Sask. Canada   \n",
      "2  All residents asked to 'shelter in place' are ...   \n",
      "3  13,000 people receive #wildfires evacuation or...   \n",
      "4  Just got sent this photo from Ruby #Alaska as ...   \n",
      "\n",
      "                                          text_clean  \n",
      "0              deeds reason earthquake allah forgive  \n",
      "1                   forest near la ronge sask canada  \n",
      "2  residents asked shelter place notified officer...  \n",
      "3  people receive wildfires evacuation orders cal...  \n",
      "4  just got sent photo ruby alaska smoke wildfire...  \n"
     ]
    }
   ],
   "source": [
    "# Cell 2 ‚Äî Preprocesamiento detallado (inciso 3) ‚Äî versi√≥n corregida\n",
    "\n",
    "import re, html, unicodedata, string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Asegurarnos de tener train_df, text_col y target_col (del Cell 1) ---\n",
    "if \"train_df\" not in globals():\n",
    "    train_df = pd.read_csv(\"train.csv\")\n",
    "if \"text_col\" not in globals():\n",
    "    text_col = \"text\" if \"text\" in train_df.columns else train_df.select_dtypes(\"object\").columns[0]\n",
    "if \"target_col\" not in globals():\n",
    "    target_col = \"target\" if \"target\" in train_df.columns else None\n",
    "\n",
    "print(f\"Usando columnas -> texto: {text_col!r} | etiqueta: {target_col!r}\")\n",
    "\n",
    "# --- Stopwords en ingl√©s (evitamos dependencias de descarga) ---\n",
    "try:\n",
    "    from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "    STOPWORDS = set(ENGLISH_STOP_WORDS)\n",
    "except Exception:\n",
    "    # Fallback m√≠nimo si sklearn no est√° disponible\n",
    "    STOPWORDS = set(\"\"\"\n",
    "a about above after again against all am an and any are as at be because been before being below between both\n",
    "but by can did do does doing down during each few for from further had has have having he she'd he'll she's her\n",
    "here hers herself him himself his how i i'd i'll i'm i've if in into is it it's its itself let me more most my\n",
    "myself no nor not of off on once only or other our ours ourselves out over own same she should so some such\n",
    "than that that's the their theirs them themselves then there there's these they they'd they'll they're they've\n",
    "this those through to too under until up very was we we'd we'll we're we've were what what's when when's where\n",
    "where's which while who who's whom why why's with you you'd you'll you're you've your yours yourself yourselves\n",
    "\"\"\".split())\n",
    "\n",
    "# Ajustes de stopwords espec√≠ficos de tweets\n",
    "STOPWORDS |= {\"rt\", \"amp\", \"https\", \"http\", \"co\", \"t\"}  # &amp; y tokens comunes de URLs acortadas\n",
    "\n",
    "# --- Compilar patrones regex reutilizables ---\n",
    "URL_RE        = re.compile(r\"(https?://\\S+|www\\.\\S+)\", flags=re.IGNORECASE)\n",
    "MENTION_RE    = re.compile(r\"@\\w+\")\n",
    "HASHTAG_RE    = re.compile(r\"#(\\w+)\")\n",
    "HTML_ENT_RE   = re.compile(r\"&[a-z]+;\")\n",
    "EMOTICON_RE   = re.compile(r\"(?::|;|=|8)(?:-|'|o)?(?:\\)|\\(|D|P|/|\\\\|\\||\\]|\\[)\", flags=re.IGNORECASE)\n",
    "\n",
    "# Rango amplio de emojis (unicode)\n",
    "EMOJI_RE = re.compile(\n",
    "    \"[\" +\n",
    "    \"\\U0001F300-\\U0001F5FF\" +  # pictos y s√≠mbolos\n",
    "    \"\\U0001F600-\\U0001F64F\" +  # emoticonos\n",
    "    \"\\U0001F680-\\U0001F6FF\" +  # transporte\n",
    "    \"\\U0001F700-\\U0001F77F\" +\n",
    "    \"\\U0001F780-\\U0001F7FF\" +\n",
    "    \"\\U0001F800-\\U0001F8FF\" +\n",
    "    \"\\U0001F900-\\U0001F9FF\" +\n",
    "    \"\\U0001FA00-\\U0001FA6F\" +\n",
    "    \"\\U0001FA70-\\U0001FAFF\" +\n",
    "    \"\\U00002700-\\U000027BF\" +  # dingbats\n",
    "    \"\\U00002600-\\U000026FF\" +  # miscel√°neo\n",
    "    \"]+\",\n",
    "    flags=re.UNICODE\n",
    ")\n",
    "\n",
    "PUNCT_TABLE = str.maketrans(\"\", \"\", string.punctuation)  # elimina puntuaci√≥n ASCII\n",
    "\n",
    "def normalize_unicode(text: str) -> str:\n",
    "    # Normaliza a NFKC para unificar formas y s√≠mbolos \"raros\"\n",
    "    return unicodedata.normalize(\"NFKC\", text)\n",
    "\n",
    "def preprocess_text(s: str, keep_number_token=\"911\"):\n",
    "    orig = str(s)\n",
    "\n",
    "    # M√©tricas de sucio\n",
    "    urls_found     = len(URL_RE.findall(orig))\n",
    "    mentions_found = len(MENTION_RE.findall(orig))\n",
    "    hashtags_found = len(HASHTAG_RE.findall(orig))\n",
    "    emojis_found   = len(EMOJI_RE.findall(orig)) + len(EMOTICON_RE.findall(orig))\n",
    "\n",
    "    # 1) Min√∫sculas + desenmascarar HTML (&amp; -> &)\n",
    "    t = html.unescape(orig).lower()\n",
    "\n",
    "    # 2) Normalizaci√≥n unicode\n",
    "    t = normalize_unicode(t)\n",
    "\n",
    "    # 3) Eliminar URLs completas\n",
    "    t = URL_RE.sub(\" \", t)\n",
    "\n",
    "    # 4) Eliminar @menciones\n",
    "    t = MENTION_RE.sub(\" \", t)\n",
    "\n",
    "    # 5) Mantener el texto del hashtag, quitando el '#'\n",
    "    #    \"#wildfire\" -> \"wildfire\"\n",
    "    t = HASHTAG_RE.sub(r\"\\1\", t)\n",
    "\n",
    "    # 6) Eliminar emojis y emoticones\n",
    "    t = EMOJI_RE.sub(\" \", t)\n",
    "    t = EMOTICON_RE.sub(\" \", t)\n",
    "\n",
    "    # 7) Eliminar puntuaci√≥n ASCII (quita ap√≥strofes, comas, etc.)\n",
    "    t = t.translate(PUNCT_TABLE)\n",
    "\n",
    "    # 8) Quitar residuos de entidades HTML restantes\n",
    "    t = HTML_ENT_RE.sub(\" \", t)\n",
    "\n",
    "    # 9) Tokenizar por espacios\n",
    "    tokens = [tok for tok in t.split() if tok]\n",
    "\n",
    "    # 10) Quitar stopwords\n",
    "    tokens = [tok for tok in tokens if tok not in STOPWORDS]\n",
    "\n",
    "    # 11) Manejo de n√∫meros:\n",
    "    #     - Eliminamos tokens NUM√âRICOS EXCEPTO \"911\" (mantener porque puede ser relevante en desastres)\n",
    "    #     - Para tokens alfanum√©ricos, conservamos tal cual (e.g., \"h2o\", \"m5.0\")\n",
    "    cleaned_tokens = []\n",
    "    for tok in tokens:\n",
    "        if tok.isdigit():\n",
    "            if keep_number_token and tok == keep_number_token:\n",
    "                cleaned_tokens.append(tok)\n",
    "            # si es otro n√∫mero puro, lo omitimos\n",
    "        else:\n",
    "            cleaned_tokens.append(tok)\n",
    "\n",
    "    # 12) Reconstrucci√≥n\n",
    "    clean_text = \" \".join(cleaned_tokens)\n",
    "\n",
    "    # M√©tricas post\n",
    "    return {\n",
    "        \"clean_text\": clean_text,\n",
    "        \"tokens\": cleaned_tokens,\n",
    "        \"n_urls\": urls_found,\n",
    "        \"n_mentions\": mentions_found,\n",
    "        \"n_hashtags\": hashtags_found,\n",
    "        \"n_emojis\": emojis_found,\n",
    "        \"orig_len_chars\": len(orig),\n",
    "        \"orig_len_tokens\": len(orig.split()),\n",
    "        \"clean_len_tokens\": len(cleaned_tokens),\n",
    "    }\n",
    "\n",
    "# --- Aplicar preprocesamiento ---\n",
    "proc = train_df[text_col].astype(str).apply(preprocess_text)\n",
    "\n",
    "train_df[\"text_clean\"]   = proc.apply(lambda x: x[\"clean_text\"])\n",
    "train_df[\"tokens_clean\"] = proc.apply(lambda x: x[\"tokens\"])\n",
    "\n",
    "# M√©tricas agregadas del proceso\n",
    "report = pd.DataFrame({\n",
    "    \"urls\":         proc.apply(lambda x: x[\"n_urls\"]),\n",
    "    \"mentions\":     proc.apply(lambda x: x[\"n_mentions\"]),\n",
    "    \"hashtags\":     proc.apply(lambda x: x[\"n_hashtags\"]),\n",
    "    \"emojis\":       proc.apply(lambda x: x[\"n_emojis\"]),\n",
    "    \"orig_chars\":   proc.apply(lambda x: x[\"orig_len_chars\"]),\n",
    "    \"orig_tokens\":  proc.apply(lambda x: x[\"orig_len_tokens\"]),\n",
    "    \"clean_tokens\": proc.apply(lambda x: x[\"clean_len_tokens\"]),\n",
    "})\n",
    "\n",
    "# --- Reporte en consola ---\n",
    "print(\"\\n=== REPORTE DE PREPROCESAMIENTO (train) ===\")\n",
    "print(f\"Filas procesadas: {len(train_df)}\")\n",
    "print(\"Totales removidos/detectados:\")\n",
    "print(\" ‚Ä¢ URLs:     \", int(report[\"urls\"].sum()))\n",
    "print(\" ‚Ä¢ Menciones:\", int(report[\"mentions\"].sum()))\n",
    "print(\" ‚Ä¢ Hashtags: \", int(report[\"hashtags\"].sum()), \"(se conserv√≥ la palabra, sin '#')\")\n",
    "print(\" ‚Ä¢ Emojis:   \", int(report[\"emojis\"].sum()))\n",
    "\n",
    "print(\"\\nLongitud de tokens (antes vs despu√©s):\")\n",
    "print(\" ‚Ä¢ Tokens (orig)  -> mean:\", round(report[\"orig_tokens\"].mean(),1),\n",
    "      \"| p50:\", int(report[\"orig_tokens\"].median()),\n",
    "      \"| p95:\", int(report[\"orig_tokens\"].quantile(0.95)))\n",
    "print(\" ‚Ä¢ Tokens (clean) -> mean:\", round(report[\"clean_tokens\"].mean(),1),\n",
    "      \"| p50:\", int(report[\"clean_tokens\"].median()),\n",
    "      \"| p95:\", int(report[\"clean_tokens\"].quantile(0.95)))\n",
    "\n",
    "# Distribuci√≥n por clase (si hay etiqueta) ‚Äî usando longitud de tokens limpios\n",
    "if target_col is not None and target_col in train_df.columns:\n",
    "    train_df[\"tok_len\"] = train_df[\"tokens_clean\"].apply(len)\n",
    "    by_cls = train_df.groupby(target_col)[\"tok_len\"].agg([\"mean\",\"median\",\"max\",\"min\"])\n",
    "    print(\"\\nTama√±o de texto limpio por clase (n√∫mero de tokens):\")\n",
    "    print(by_cls.round(2))\n",
    "\n",
    "# --- Ejemplos antes/despu√©s por clase (para documentaci√≥n del inciso 3) ---\n",
    "def show_examples(df, k=3):\n",
    "    groups = df.groupby(target_col) if (target_col is not None and target_col in df.columns) else [(None, df)]\n",
    "    for cls_val, g in groups:\n",
    "        print(f\"\\n>> Ejemplos clase={cls_val}:\")\n",
    "        samp = g.sample(n=min(k, len(g)), random_state=42)\n",
    "        for _, r in samp.iterrows():\n",
    "            raw = str(r[text_col]).strip().replace(\"\\n\", \" \")\n",
    "            clean = str(r[\"text_clean\"]).strip()\n",
    "            print(\"- RAW  :\", raw[:160] + (\"...\" if len(raw) > 160 else \"\"))\n",
    "            print(\"  CLEAN:\", clean[:160] + (\"...\" if len(clean) > 160 else \"\"))\n",
    "\n",
    "show_examples(train_df, k=3)\n",
    "\n",
    "# --- Chequeo r√°pido de vac√≠os tras limpieza (p.ej. posts que quedan sin tokens) ---\n",
    "empties = train_df[\"text_clean\"].fillna(\"\").str.strip().eq(\"\")\n",
    "n_empties = int(empties.sum())\n",
    "print(f\"\\nPublicaciones que quedaron vac√≠as tras limpieza: {n_empties}\")\n",
    "\n",
    "# --- Vista previa final ---\n",
    "print(\"\\n=== Vista previa columnas limpias ===\")\n",
    "print(train_df[[text_col, \"text_clean\"]].head(5))\n",
    "\n",
    "# Nota: NO eliminamos duplicados aqu√≠ para no alterar el dataset base;\n",
    "# si lo necesitas para el modelo, podemos hacerlo en el inciso del modelado.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56dcc9f",
   "metadata": {},
   "source": [
    "\n",
    "# Inciso 3 ‚Äî An√°lisis del preprocesamiento (train.csv)\n",
    "\n",
    "**Columnas usadas** ‚Üí texto: `text` | etiqueta: `target`  \n",
    "**Filas procesadas**: 7,613\n",
    "\n",
    "---\n",
    "\n",
    "## 1) Qu√© se limpi√≥/elimin√≥\n",
    "\n",
    "- **URLs** detectadas: **4,723**  \n",
    "- **@Menciones**: **2,715**  \n",
    "- **#Hashtags**: **3,330** *(se conserva la palabra, sin ‚Äò#‚Äô)*  \n",
    "- **Emojis/Emoticones**: **4,888**\n",
    "\n",
    "> Comentario: El dataset tiene una **alta carga social** (muchas URLs, menciones y emojis). Limpiar estos elementos ayuda a reducir ruido y sesgos hacia la forma de escritura en Twitter sin eliminar se√±ales sem√°nticas clave (p. ej., *wildfire*, *evacuation*, *floods*).\n",
    "\n",
    "---\n",
    "\n",
    "## 2) Longitud de texto (antes vs despu√©s)\n",
    "\n",
    "- **Tokens (original)** ‚Üí *mean*: **14.9**, *p50*: **15**, *p95*: **24**  \n",
    "- **Tokens (limpio)** ‚Üí *mean*: **8.2**, *p50*: **8**, *p95*: **14**\n",
    "\n",
    "> Interpretaci√≥n: tras la limpieza, el texto queda **~45% m√°s corto** en promedio. Esto sugiere que gran parte del contenido eran conectores, signos, URLs y marcas sociales. La reducci√≥n es esperable y suele **mejorar la densidad sem√°ntica** por token.\n",
    "\n",
    "---\n",
    "\n",
    "## 3) Longitud por clase (tokens limpios)\n",
    "\n",
    "| target | mean | median | max | min |\n",
    "|:------:|-----:|-------:|----:|----:|\n",
    "|   0    | 7.77 |  8.00  | 20  |  0  |\n",
    "|   1    | 8.77 |  9.00  | 21  |  0  |\n",
    "\n",
    "> Observaci√≥n: Los tweets **etiquetados como desastre (1)** son, en promedio, **~1 token m√°s largos**. No es una diferencia enorme, pero puede indicar que los tweets de desastres incluyen **m√°s contexto** (p. ej., lugar + evento + afectaci√≥n).\n",
    "\n",
    "---\n",
    "\n",
    "## 4) Ejemplos ilustrativos (antes ‚Üí despu√©s)\n",
    "\n",
    "**Clase 0 (no desastre):**\n",
    "- *everyday is a near death fatality‚Ä¶* ‚Üí `everyday near death fatality road thank god`  \n",
    "  - Riesgo de **falsos positivos**: palabras de alto ‚Äúdrama‚Äù (*death*, *fatality*) en contextos figurados o hiperbolizados.\n",
    "- *#Lifestyle‚Ä¶ hazard* ‚Üí `lifestyle ‚Ä¶ makes sick ‚Ä¶ baby clothes deemed ‚Ä¶ hazard`  \n",
    "  - Muestra ruido de codificaci√≥n (ver ‚Äúmojibake‚Äù abajo).\n",
    "- *@Lenn_Len‚Ä¶ inundated* ‚Üí `probably inundated years`  \n",
    "  - *inundated* aqu√≠ no describe una **inundaci√≥n real**, sino ‚Äúabrumados‚Äù ‚Üí **ambig√ºedad sem√°ntica**.\n",
    "\n",
    "**Clase 1 (desastre):**\n",
    "- *‚Ä¶floods in #Paraguay* ‚Üí `nearly thousand people affected floods paraguay`  \n",
    "  - L√©xico fuerte: *affected*, *floods*, **top√≥nimo** (*paraguay*).\n",
    "- *‚Ä¶escape Armageddon* ‚Üí `vladimir putin issues major warning late escape armageddon`  \n",
    "  - Palabras catastr√≥ficas (no siempre desastre natural, pero se√±alan severidad).\n",
    "- *@‚Ä¶ burning buildings‚Ä¶ riot* ‚Üí `burning buildings rob riot thats embarrassing ruining nation`  \n",
    "  - L√©xico de **eventos violentos/da√±o** (*burning*, *buildings*, *riot*).\n",
    "\n",
    "---\n",
    "\n",
    "## 5) Calidad de limpieza: hallazgos y mejoras sugeridas\n",
    "\n",
    "- **4 publicaciones vac√≠as** tras limpieza ‚Üí recomendaci√≥n: *drop* o marcar para tratamiento especial (no aportan se√±al textual).  \n",
    "- **Mojibake / artefactos Unicode** (p. ej., `¬â√ª√∑`) persisten en algunos textos.\n",
    "  - *Sugerencia*: a√±adir una etapa opcional para **filtrar caracteres no alfab√©ticos** fuera de rangos comunes o normalizar a ASCII cuando el idioma sea ingl√©s:\n",
    "    - Ej.: `re.sub(r\"[^a-z0-9\\s\\-]\", \" \", text)` tras NFKC (cuidar no perder top√≥nimos con tildes si hubiera idioma mixto).\n",
    "- **N√∫meros**: se conserv√≥ **‚Äú911‚Äù** y se eliminaron otros n√∫meros puros.\n",
    "  - *Sugerencia*: adem√°s de ‚Äú911‚Äù, puede valer la pena **conservar n√∫meros decimales** y patrones de magnitud (*‚Äú5.8‚Äù*, *‚Äúm5.0‚Äù*) por su relaci√≥n con **terremotos**.\n",
    "  - Regla posible: mantener `r\"\\d+\\.\\d+\"` (decimales) y tokens con letras+digits (*m5.0, h2o* ya se conservan).\n",
    "\n",
    "---\n",
    "\n",
    "## 6) Implicaciones para unigramas/bigramas (inciso 4)\n",
    "\n",
    "- **Unigramas** √∫tiles (intuici√≥n por ejemplos): `wildfire`, `evacuation`, `shelter`, `floods`, `smoke`, `burning`, `evacuate`, `earthquake`, `aftershock`, `landslide`, `tornado`, `explosion`, `derailment`, `casualties`, `emergency`, `fatalities`, `rescue`.  \n",
    "- **Bigrams** muy recomendables para **contexto**:\n",
    "  - `forest fire`, `car crash`, `shelter in`, `in place`, `evacuation orders`, `burning buildings`, `state emergency`, `death toll`, `flash flood`, `wildfire smoke`.  \n",
    "  - Tambi√©n **top√≥nimo + evento**: `paraguay floods`, `alaska wildfire`, etc.\n",
    "- **Trigrams** puntuales: `shelter in place`, `state of emergency`.  \n",
    "  - √ötiles pero m√°s escasos; probar **bigrams primero** y evaluar mejora.\n",
    "\n",
    "> Conclusi√≥n: s√≠ **vale la pena explorar bigramas** (y unos pocos trigrams clave) para capturar **frases indicadoras** que desambiguÃàen palabras sueltas.\n",
    "\n",
    "---\n",
    "\n",
    "## 7) Palabras con riesgo de confusi√≥n (discutir en el reporte)\n",
    "\n",
    "- **Hiperboles cotidianas** (*‚Äúnear death‚Äù*, *‚Äúbombed this exam‚Äù*, *‚Äúmy phone exploded‚Äù*): pueden **sobrerreaccionar** en unigramas.  \n",
    "  - Bigrams/trigrams y se√±ales **de contexto real** (lugares, cifras, verbos de reporte: *evacuate, rescue, declare*) ayudan a reducir falsos positivos.\n",
    "- **Polisemia** (*inundated* = abrumado vs inundado real).  \n",
    "  - **Top√≥nimos**, **fechas/horas**, **n√∫meros de v√≠ctimas**, y palabras de **autoridad** (*officials, declared, warning, alert*) tienden a correlacionar mejor con eventos reales.\n",
    "\n",
    "---\n",
    "\n",
    "## 8) Recomendaciones pr√°cticas para el modelo preliminar\n",
    "\n",
    "1. **Features**: TF‚ÄìIDF **unigramas+bigramas** con min_df bajo (p. ej., 2‚Äì5) y `max_features` razonable (20k‚Äì50k), n-gram range `(1,2)`.  \n",
    "2. **Clase desbalanceada moderada** (57/43): probar **calibraci√≥n de clase** (`class_weight='balanced'` en modelos lineales) y **AUC/F1** como m√©tricas.  \n",
    "3. **Deduplicaci√≥n opcional**: hay **110 textos duplicados**; considerar *drop* para evitar sesgos en validaci√≥n.  \n",
    "4. **Validaci√≥n**: *stratified split* y baseline con **Logistic Regression** / **Linear SVM**; comparar con Naive Bayes.  \n",
    "5. **Top tokens por clase**: antes del modelo, obtener **frecuencias por clase** y **nubes de palabras** para documentar (inciso 4 y 5).\n",
    "\n",
    "---\n",
    "\n",
    "## 9) Vista previa (sanity check)\n",
    "\n",
    "Se observa que el **contenido sem√°ntico central** se conserva, p. ej.:  \n",
    "- `deeds reason earthquake allah forgive`  \n",
    "- `residents asked shelter place ‚Ä¶`  \n",
    "- `people receive wildfires evacuation orders california ‚Ä¶`  \n",
    "- `smoke wildfire ‚Ä¶ alaska ‚Ä¶`  \n",
    "\n",
    "> Esto confirma que la limpieza **respeta los t√©rminos clave** asociados a desastres y reduce ruido social.\n",
    "\n",
    "---\n",
    "\n",
    "### Pr√≥ximo paso\n",
    "Proceder con **inciso 4**: conteos de frecuencia **por clase** (unigramas y bigramas), discusi√≥n de t√©rminos √∫tiles y visualizaciones (nube de palabras + histogramas).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
